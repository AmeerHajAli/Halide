To generate results for master, greedy, beam, and mcts:
    $ ./generate_all_apps_results.sh [--improved/--value_func]

You can also edit line 1 of ./generate_all_apps_results.sh if you only want to run a subset of the tests.

To get the autotuning results (without retraining):
    $ RETRAIN=false ./generate_autotune_results.sh beam [--improved/--value_func]

To get the autotuning results with retraining:
    $ RETRAIN=true ./generate_autotune_results.sh beam [--improved/--value_func]

The commands above will run autotuning using the "beam" autotuner, which is what the original code in the "standalone_autoscheduler" branch appeared to do. However, if you want to use another autoscheduler while doing autotuning, you can, by replacing "beam" in the command above with "greedy", or "mcts". We have not yet added the ability to autotune with "master".

Afterwards, you can just run through the Jupyter notebook in "plots/" to get the charts. You can select in the notebook whether you want to plot the autotuning results as well, in the very first cell of the notebook.

For all the commands given above, the optional "--improved" argument will run using the improved weights (found in "apps/autoscheduler/improved.weights") rather than the baseline weights.
The optional "--value_func" argument will run using the value function weights (found in "apps/autoscheduler/value_func.weights") rather than the baseline weights.

To retrain the cost model on the random pipelines, run the following command:
    $ ./train_cost_model.sh beam

Afterwards, the updated weights will be found in apps/random_pipeline/samples/updated.weights.

Instead, you may wish to retrain the cost model on the benchmark applications themselves. To do so, run the following command:
    $ ./train_cost_model_on_benchmarks.sh beam

Afterwards, the updated weights will be found in apps/autoscheduler/paper_results_scripts/samples/updated.weights.

